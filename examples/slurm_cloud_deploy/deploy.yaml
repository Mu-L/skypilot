resources:
  cloud: aws
  cpus: 4+
  disk_size: 50
  # Uncomment the next line to request GPUs
  # accelerators: T4:4  # Can be changed as needed (e.g., A100, T4, etc.)

num_nodes: 2  # Can be adjusted based on your needs

# Using block scalar with chomping indicator (|) for the script
run: |
  set -ex
  sudo apt update
  sudo apt install -y munge slurm-wlm slurm-wlm-basic-plugins slurmctld slurmd

  # Skip NVIDIA driver installation as it's typically pre-installed on GPU instances
  # If on a GPU instance, report driver status
  if command -v nvidia-smi &> /dev/null; then
    echo "Using pre-installed NVIDIA drivers"
    nvidia-smi --query-gpu=driver_version --format=csv,noheader
  fi

  CONTROLLER_IP=$(echo "$SKYPILOT_NODE_IPS" | head -n1)
  # Get username - works on both AWS (ubuntu) and other clouds
  USERNAME=$(whoami)

  # Define barrier function
  create_barrier() {
    local barrier_name=$1
    local barrier_file="/tmp/${barrier_name}${SKYPILOT_TASK_ID}_barrier"
    
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # Controller node creates and distributes barrier file
      touch $barrier_file
      for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
        scp -o StrictHostKeyChecking=no $barrier_file $node:$barrier_file
      done
    else
      # Non-controller nodes wait for barrier file
      while [ ! -f "$barrier_file" ]; do
        echo "Waiting for $barrier_name barrier..."
        sleep 5
      done
    fi
  }

  # Generate munge key manually on controller
  if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
    sudo sh -c 'dd if=/dev/urandom bs=1 count=1024 > /etc/munge/munge.key'
    sudo chown munge:munge /etc/munge/munge.key
    sudo chmod 400 /etc/munge/munge.key

    # Copy munge.key temporarily for scp
    sudo cp /etc/munge/munge.key /tmp/munge.key
    sudo chown $USERNAME:$USERNAME /tmp/munge.key

    # Now you can scp from /tmp/munge.key
    for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
      scp -o StrictHostKeyChecking=no /tmp/munge.key $node:/tmp/munge.key
      ssh -o StrictHostKeyChecking=no $node "sudo mv /tmp/munge.key /etc/munge/munge.key && sudo chown munge:munge /etc/munge/munge.key && sudo chmod 400 /etc/munge/munge.key"
    done

    # cleanup
    sudo rm /tmp/munge.key
  fi
  
  # Create barrier to ensure munge key is distributed before continuing
  create_barrier "munge_key_distributed"

  sudo systemctl enable munge
  sudo systemctl restart munge

  echo "Nodes: $SKYPILOT_NODE_IPS"

  # Create and distribute slurm.conf and gres.conf from controller
  if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
    sudo mkdir -p /var/spool/slurmctld
    sudo chown slurm:slurm /var/spool/slurmctld

    CONTROLLER_HOSTNAME=$(hostname)
    CONTROLLER_IP=$(echo "$SKYPILOT_NODE_IPS" | head -n1)

    # Detect GPU type on controller node (for information only)
    GPU_TYPE=""
    if command -v nvidia-smi &> /dev/null; then
      gpu_count=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
      if [ $gpu_count -gt 0 ]; then
        GPU_TYPE=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -n1 | sed 's/ /_/g')
        echo "Detected GPU type: $GPU_TYPE (will be configured as generic 'gpu' resources)"
      fi
    fi

    # Initialize slurm.conf with tee command instead of heredoc
    sudo bash -c "cat > /etc/slurm/slurm.conf" << 'EOF'
  ClusterName=skypilot_cluster
  SlurmctldHost=CONTROLLER_HOSTNAME_PLACEHOLDER
  SlurmUser=slurm
  StateSaveLocation=/var/spool/slurmctld
  SlurmdSpoolDir=/var/spool/slurmd
  AuthType=auth/munge
  SelectType=select/cons_res
  SelectTypeParameters=CR_Core_Memory
  # Use Linux process tracking instead of cgroups
  ProctrackType=proctrack/linuxproc
  # Enable GPU support via Generic Resources (if GPUs are present)
  GresTypes=gpu
  # Enable accounting
  AccountingStorageType=accounting_storage/none
  JobAcctGatherType=jobacct_gather/none
  EOF

    # Replace the placeholder with actual hostname
    sudo sed -i "s/CONTROLLER_HOSTNAME_PLACEHOLDER/$CONTROLLER_HOSTNAME/" /etc/slurm/slurm.conf

    # Create empty gres.conf to be populated if GPUs are detected
    echo "# GPU configuration for Slurm" | sudo tee /etc/slurm/gres.conf

    # Initialize a nodeinfo array to store all node info for partitioning later
    declare -a nodeinfo=()

    # Dynamically append Node definitions
    RANK=0
    while read -r node; do
      if [ $RANK -eq 0 ]; then
        node_hostname=$CONTROLLER_HOSTNAME
        # Get actual CPU count for controller node
        unset OMP_NUM_THREADS
        node_cpus=$(nproc)
        # Get more detailed CPU information
        node_sockets=$(lscpu | grep "Socket(s):" | awk '{print $2}')
        node_cores_per_socket=$(lscpu | grep "Core(s) per socket:" | awk '{print $4}')
        node_threads_per_core=$(lscpu | grep "Thread(s) per core:" | awk '{print $4}')
        
        # Check for GPUs on controller node
        if command -v nvidia-smi &> /dev/null; then
          gpu_count=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
          if [ $gpu_count -gt 0 ]; then
            # Add Gres information to slurm.conf node entry - using generic 'gpu' type
            node_gres="Gres=gpu:${gpu_count}"
            
            # Add controller's GPU configuration to gres.conf - using generic 'gpu' type
            for gpu_id in $(seq 0 $((gpu_count-1))); do
              echo "NodeName=${node_hostname} Name=gpu File=/dev/nvidia${gpu_id}" | sudo tee -a /etc/slurm/gres.conf
            done
          else
            node_gres=""
          fi
        else
          node_gres=""
        fi
        
        # Store node info for partitioning
        nodeinfo+=("$node_hostname:$gpu_count")
      else
        node_hostname=$(ssh -o StrictHostKeyChecking=no $node hostname </dev/null)
        # Get actual CPU count for compute nodes
        node_cpus=$(ssh -o StrictHostKeyChecking=no $node nproc </dev/null)
        # Get more detailed CPU information
        node_sockets=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Socket(s):\" | awk '{print \$2}'" </dev/null)
        node_cores_per_socket=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Core(s) per socket:\" | awk '{print \$4}'" </dev/null)
        node_threads_per_core=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Thread(s) per core:\" | awk '{print \$4}'" </dev/null)
        
        # Check only for GPU count on remote node
        if ssh -o StrictHostKeyChecking=no $node "command -v nvidia-smi" &> /dev/null; then
          gpu_count=$(ssh -o StrictHostKeyChecking=no $node "nvidia-smi --query-gpu=name --format=csv,noheader | wc -l" </dev/null)
          if [ $gpu_count -gt 0 ]; then
            # Add Gres information to slurm.conf node entry - using generic 'gpu' type
            node_gres="Gres=gpu:${gpu_count}"
            
            # Add node's GPU configuration to gres.conf - using generic 'gpu' type
            for gpu_id in $(seq 0 $((gpu_count-1))); do
              echo "NodeName=${node_hostname} Name=gpu File=/dev/nvidia${gpu_id}" | sudo tee -a /etc/slurm/gres.conf
            done
          else
            node_gres=""
          fi
        else
          node_gres=""
        fi
        
        # Store node info for partitioning
        nodeinfo+=("$node_hostname:$gpu_count")
      fi
      
      echo "Adding node $RANK: $node_hostname with IP $node. All nodes: $SKYPILOT_NODE_IPS"
      if [ -n "$node_gres" ]; then
        printf "NodeName=%s NodeAddr=%s CPUs=%s Sockets=%s CoresPerSocket=%s ThreadsPerCore=%s %s State=UNKNOWN\n" "${node_hostname}" "${node}" "${node_cpus}" "${node_sockets}" "${node_cores_per_socket}" "${node_threads_per_core}" "${node_gres}" | sudo tee -a /etc/slurm/slurm.conf
      else
        printf "NodeName=%s NodeAddr=%s CPUs=%s Sockets=%s CoresPerSocket=%s ThreadsPerCore=%s State=UNKNOWN\n" "${node_hostname}" "${node}" "${node_cpus}" "${node_sockets}" "${node_cores_per_socket}" "${node_threads_per_core}" | sudo tee -a /etc/slurm/slurm.conf
      fi
      RANK=$((RANK+1))
    done <<< "$SKYPILOT_NODE_IPS"

    # Create partition definitions based on GPU availability
    gpu_nodes=""
    cpu_nodes=""
    
    for node_info in "${nodeinfo[@]}"; do
      node_name=$(echo "$node_info" | cut -d: -f1)
      node_gpus=$(echo "$node_info" | cut -d: -f2)
      
      if [ "$node_gpus" -gt 0 ]; then
        if [ -z "$gpu_nodes" ]; then
          gpu_nodes="$node_name"
        else
          gpu_nodes="$gpu_nodes,$node_name"
        fi
      fi
      
      if [ -z "$cpu_nodes" ]; then
        cpu_nodes="$node_name"
      else
        cpu_nodes="$cpu_nodes,$node_name"
      fi
    done
    
    # Create partitions
    echo "PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP" | sudo tee -a /etc/slurm/slurm.conf
    
    # Create GPU partition if GPU nodes exist
    if [ -n "$gpu_nodes" ]; then
      echo "PartitionName=gpu Nodes=${gpu_nodes} Default=NO MaxTime=INFINITE State=UP" | sudo tee -a /etc/slurm/slurm.conf
    fi

    # Distribute slurm.conf and gres.conf to all nodes
    for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
      scp -o StrictHostKeyChecking=no /etc/slurm/slurm.conf $node:/tmp/
      ssh $node "sudo mv /tmp/slurm.conf /etc/slurm/slurm.conf"
      
      scp -o StrictHostKeyChecking=no /etc/slurm/gres.conf $node:/tmp/
      ssh $node "sudo mv /tmp/gres.conf /etc/slurm/gres.conf"
    done

    sudo systemctl stop slurmctld || true
    sudo systemctl enable slurmctld
    sudo systemctl start slurmctld
  fi
  # Create barrier to ensure slurm.conf is distributed before continuing
  create_barrier "slurm_conf_distributed"

  sudo mkdir -p /var/spool/slurmd
  sudo chown slurm:slurm /var/spool/slurmd
  sudo systemctl stop slurmd || true
  sudo systemctl enable slurmd
  sudo systemctl start slurmd
  
  # Show cluster configuration and GPU status if applicable
  echo "=== Slurm Cluster Status ==="
  sinfo
  
  if command -v nvidia-smi &> /dev/null; then
    echo "=== GPU Status ==="
    nvidia-smi
    echo "=== Available GPU Resources in Slurm ==="
    scontrol show nodes | grep "Gres="
    
    echo "=== Example GPU job submission ==="
    echo "To request a GPU job: sbatch --gres=gpu:1 --partition=gpu my_gpu_job.sh"
    echo "To request multiple GPUs: sbatch --gres=gpu:4 --partition=gpu my_gpu_job.sh"
  else
    echo "=== Example CPU job submission ==="
    echo "To submit a job: sbatch my_job.sh"
  fi 
