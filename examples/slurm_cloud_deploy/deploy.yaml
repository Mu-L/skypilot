resources:
  cloud: aws
  cpus: 4+
  disk_size: 50

num_nodes: 4

run: |
  set -ex
  sudo apt update
  sudo apt install -y munge slurm-wlm slurm-wlm-basic-plugins slurmctld slurmd

  CONTROLLER_IP=$(echo "$SKYPILOT_NODE_IPS" | head -n1)

  # Define barrier function
  create_barrier() {
    local barrier_name=$1
    local barrier_file="/tmp/${barrier_name}${SKYPILOT_TASK_ID}_barrier"
    
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # Controller node creates and distributes barrier file
      touch $barrier_file
      for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
        scp -o StrictHostKeyChecking=no $barrier_file $node:$barrier_file
      done
    else
      # Non-controller nodes wait for barrier file
      while [ ! -f "$barrier_file" ]; do
        echo "Waiting for $barrier_name barrier..."
        sleep 5
      done
    fi
  }

  # Generate munge key manually on controller
  if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
    sudo sh -c 'dd if=/dev/urandom bs=1 count=1024 > /etc/munge/munge.key'
    sudo chown munge:munge /etc/munge/munge.key
    sudo chmod 400 /etc/munge/munge.key

    # Copy munge.key temporarily for scp
    sudo cp /etc/munge/munge.key /tmp/munge.key
    sudo chown ubuntu:ubuntu /tmp/munge.key

    # Now you can scp from /tmp/munge.key
    for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
      scp -o StrictHostKeyChecking=no /tmp/munge.key ubuntu@$node:/tmp/munge.key
      ssh -o StrictHostKeyChecking=no ubuntu@$node "sudo mv /tmp/munge.key /etc/munge/munge.key && sudo chown munge:munge /etc/munge/munge.key && sudo chmod 400 /etc/munge/munge.key"
    done

    # cleanup
    sudo rm /tmp/munge.key
  fi
  
  # Create barrier to ensure munge key is distributed before continuing
  create_barrier "munge_key_distributed"

  sudo systemctl stop munge || true
  sudo systemctl enable munge
  sudo systemctl start munge

  echo "Nodes: $SKYPILOT_NODE_IPS"

  # Create and distribute slurm.conf from controller
  if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
    sudo mkdir -p /var/spool/slurmctld
    sudo chown slurm:slurm /var/spool/slurmctld

    CONTROLLER_HOSTNAME=$(hostname)
    CONTROLLER_IP=$(echo "$SKYPILOT_NODE_IPS" | head -n1)

    # Initialize slurm.conf
    sudo sh -c 'cat << EOF > /etc/slurm/slurm.conf
  ClusterName=skypilot_cluster
  SlurmctldHost='${CONTROLLER_HOSTNAME}'
  SlurmUser=slurm
  StateSaveLocation=/var/spool/slurmctld
  SlurmdSpoolDir=/var/spool/slurmd
  AuthType=auth/munge
  SelectType=select/cons_res
  SelectTypeParameters=CR_Core
  # Use Linux process tracking instead of cgroups
  ProctrackType=proctrack/linuxproc
  EOF'

    # Dynamically append Node definitions
    RANK=0
    while read -r node; do
      if [ $RANK -eq 0 ]; then
        node_hostname=$CONTROLLER_HOSTNAME
        # Get actual CPU count for controller node
        unset OMP_NUM_THREADS
        node_cpus=$(nproc)
        # Get more detailed CPU information
        node_sockets=$(lscpu | grep "Socket(s):" | awk '{print $2}')
        node_cores_per_socket=$(lscpu | grep "Core(s) per socket:" | awk '{print $4}')
        node_threads_per_core=$(lscpu | grep "Thread(s) per core:" | awk '{print $4}')
      else
        node_hostname=$(ssh -o StrictHostKeyChecking=no $node hostname </dev/null)
        # Get actual CPU count for compute nodes
        node_cpus=$(ssh -o StrictHostKeyChecking=no $node nproc </dev/null)
        # Get more detailed CPU information
        node_sockets=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Socket(s):\" | awk '{print \$2}'" </dev/null)
        node_cores_per_socket=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Core(s) per socket:\" | awk '{print \$4}'" </dev/null)
        node_threads_per_core=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Thread(s) per core:\" | awk '{print \$4}'" </dev/null)
      fi
      echo "Adding node $RANK: $node_hostname with IP $node. All nodes: $SKYPILOT_NODE_IPS"
      sudo sh -c "printf 'NodeName=%s NodeAddr=%s CPUs=%s Sockets=%s CoresPerSocket=%s ThreadsPerCore=%s State=UNKNOWN\n' ${node_hostname} ${node} ${node_cpus} ${node_sockets} ${node_cores_per_socket} ${node_threads_per_core} >> /etc/slurm/slurm.conf"
      RANK=$((RANK+1))
    done <<< "$SKYPILOT_NODE_IPS"

    # Append Partition definition dynamically with hostnames
    sudo sh -c "echo \"PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP\" >> /etc/slurm/slurm.conf"

    # Distribute slurm.conf to all nodes
    for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
      scp -o StrictHostKeyChecking=no /etc/slurm/slurm.conf $node:/tmp/
      ssh $node "sudo mv /tmp/slurm.conf /etc/slurm/slurm.conf"
    done

    sudo systemctl stop slurmctld || true
    sudo systemctl enable slurmctld
    sudo systemctl start slurmctld
  fi
  # Create barrier to ensure slurm.conf is distributed before continuing
  create_barrier "slurm_conf_distributed"

  sudo mkdir -p /var/spool/slurmd
  sudo chown slurm:slurm /var/spool/slurmd
  sudo systemctl stop slurmd || true
  sudo systemctl enable slurmd
  sudo systemctl start slurmd
  sinfo
