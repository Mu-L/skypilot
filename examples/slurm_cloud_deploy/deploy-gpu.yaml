resources:
  cloud: aws
  cpus: 4+
  disk_size: 50
  accelerators: T4:4  # Add GPU specification - can be changed as needed (e.g., A100, T4, etc.)

num_nodes: 2

run: |
  set -ex
  sudo apt update
  sudo apt install -y munge slurm-wlm slurm-wlm-basic-plugins slurmctld slurmd

  # Skip NVIDIA driver installation as it's already installed
  echo "Using pre-installed NVIDIA drivers"

  CONTROLLER_IP=$(echo "$SKYPILOT_NODE_IPS" | head -n1)

  # Define barrier function
  create_barrier() {
    local barrier_name=$1
    local barrier_file="/tmp/${barrier_name}${SKYPILOT_TASK_ID}_barrier"
    
    if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
      # Controller node creates and distributes barrier file
      touch $barrier_file
      for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
        scp -o StrictHostKeyChecking=no $barrier_file $node:$barrier_file
      done
    else
      # Non-controller nodes wait for barrier file
      while [ ! -f "$barrier_file" ]; do
        echo "Waiting for $barrier_name barrier..."
        sleep 5
      done
    fi
  }

  # Generate munge key manually on controller
  if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
    sudo sh -c 'dd if=/dev/urandom bs=1 count=1024 > /etc/munge/munge.key'
    sudo chown munge:munge /etc/munge/munge.key
    sudo chmod 400 /etc/munge/munge.key

    # Copy munge.key temporarily for scp
    sudo cp /etc/munge/munge.key /tmp/munge.key
    sudo chown ubuntu:ubuntu /tmp/munge.key

    # Now you can scp from /tmp/munge.key
    for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
      scp -o StrictHostKeyChecking=no /tmp/munge.key ubuntu@$node:/tmp/munge.key
      ssh -o StrictHostKeyChecking=no ubuntu@$node "sudo mv /tmp/munge.key /etc/munge/munge.key && sudo chown munge:munge /etc/munge/munge.key && sudo chmod 400 /etc/munge/munge.key"
    done

    # cleanup
    sudo rm /tmp/munge.key
  fi
  
  # Create barrier to ensure munge key is distributed before continuing
  create_barrier "munge_key_distributed"

  sudo systemctl stop munge || true
  sudo systemctl enable munge
  sudo systemctl start munge

  echo "Nodes: $SKYPILOT_NODE_IPS"

  # Create and distribute slurm.conf and gres.conf from controller
  if [ "$SKYPILOT_NODE_RANK" = "0" ]; then
    sudo mkdir -p /var/spool/slurmctld
    sudo chown slurm:slurm /var/spool/slurmctld

    CONTROLLER_HOSTNAME=$(hostname)
    CONTROLLER_IP=$(echo "$SKYPILOT_NODE_IPS" | head -n1)

    # Detect GPU type on controller node (for information only)
    GPU_TYPE=""
    if command -v nvidia-smi &> /dev/null; then
      gpu_count=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
      if [ $gpu_count -gt 0 ]; then
        GPU_TYPE=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -n1 | sed 's/ /_/g')
        echo "Detected GPU type: $GPU_TYPE (will be configured as generic 'gpu' resources)"
      fi
    fi

    # Initialize slurm.conf
    sudo sh -c 'cat << EOF > /etc/slurm/slurm.conf
  ClusterName=skypilot_cluster
  SlurmctldHost='${CONTROLLER_HOSTNAME}'
  SlurmUser=slurm
  StateSaveLocation=/var/spool/slurmctld
  SlurmdSpoolDir=/var/spool/slurmd
  AuthType=auth/munge
  SelectType=select/cons_res
  SelectTypeParameters=CR_Core_Memory
  # Use Linux process tracking instead of cgroups
  ProctrackType=proctrack/linuxproc
  # Enable GPU support via Generic Resources
  GresTypes=gpu
  # Enable accounting
  AccountingStorageType=accounting_storage/none
  JobAcctGatherType=jobacct_gather/none
  EOF'

    # Create empty gres.conf to be populated later
    sudo sh -c 'echo "# GPU configuration for Slurm" > /etc/slurm/gres.conf'

    # Initialize a nodeinfo array to store all node info for partitioning later
    declare -a nodeinfo=()

    # Dynamically append Node definitions
    RANK=0
    while read -r node; do
      if [ $RANK -eq 0 ]; then
        node_hostname=$CONTROLLER_HOSTNAME
        # Get actual CPU count for controller node
        unset OMP_NUM_THREADS
        node_cpus=$(nproc)
        # Get more detailed CPU information
        node_sockets=$(lscpu | grep "Socket(s):" | awk '{print $2}')
        node_cores_per_socket=$(lscpu | grep "Core(s) per socket:" | awk '{print $4}')
        node_threads_per_core=$(lscpu | grep "Thread(s) per core:" | awk '{print $4}')
        
        # Check for GPUs on controller node
        if command -v nvidia-smi &> /dev/null; then
          gpu_count=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
          if [ $gpu_count -gt 0 ]; then
            # Add Gres information to slurm.conf node entry - using generic 'gpu' type
            node_gres="Gres=gpu:${gpu_count}"
            
            # Add controller's GPU configuration to gres.conf - using generic 'gpu' type
            for gpu_id in $(seq 0 $((gpu_count-1))); do
              sudo sh -c "echo \"NodeName=${node_hostname} Name=gpu File=/dev/nvidia${gpu_id}\" >> /etc/slurm/gres.conf"
            done
          else
            node_gres=""
          fi
        else
          node_gres=""
        fi
        
        # Store node info for partitioning
        nodeinfo+=("$node_hostname:$gpu_count")
      else
        node_hostname=$(ssh -o StrictHostKeyChecking=no $node hostname </dev/null)
        # Get actual CPU count for compute nodes
        node_cpus=$(ssh -o StrictHostKeyChecking=no $node nproc </dev/null)
        # Get more detailed CPU information
        node_sockets=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Socket(s):\" | awk '{print \$2}'" </dev/null)
        node_cores_per_socket=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Core(s) per socket:\" | awk '{print \$4}'" </dev/null)
        node_threads_per_core=$(ssh -o StrictHostKeyChecking=no $node "lscpu | grep \"Thread(s) per core:\" | awk '{print \$4}'" </dev/null)
        
        # Check only for GPU count on remote node
        if ssh -o StrictHostKeyChecking=no $node "command -v nvidia-smi" &> /dev/null; then
          gpu_count=$(ssh -o StrictHostKeyChecking=no $node "nvidia-smi --query-gpu=name --format=csv,noheader | wc -l" </dev/null)
          if [ $gpu_count -gt 0 ]; then
            # Add Gres information to slurm.conf node entry - using generic 'gpu' type
            node_gres="Gres=gpu:${gpu_count}"
            
            # Add node's GPU configuration to gres.conf - using generic 'gpu' type
            for gpu_id in $(seq 0 $((gpu_count-1))); do
              sudo sh -c "echo \"NodeName=${node_hostname} Name=gpu File=/dev/nvidia${gpu_id}\" >> /etc/slurm/gres.conf"
            done
          else
            node_gres=""
          fi
        else
          node_gres=""
        fi
        
        # Store node info for partitioning
        nodeinfo+=("$node_hostname:$gpu_count")
      fi
      
      echo "Adding node $RANK: $node_hostname with IP $node. All nodes: $SKYPILOT_NODE_IPS"
      if [ -n "$node_gres" ]; then
        sudo sh -c "printf 'NodeName=%s NodeAddr=%s CPUs=%s Sockets=%s CoresPerSocket=%s ThreadsPerCore=%s %s State=UNKNOWN\n' ${node_hostname} ${node} ${node_cpus} ${node_sockets} ${node_cores_per_socket} ${node_threads_per_core} ${node_gres} >> /etc/slurm/slurm.conf"
      else
        sudo sh -c "printf 'NodeName=%s NodeAddr=%s CPUs=%s Sockets=%s CoresPerSocket=%s ThreadsPerCore=%s State=UNKNOWN\n' ${node_hostname} ${node} ${node_cpus} ${node_sockets} ${node_cores_per_socket} ${node_threads_per_core} >> /etc/slurm/slurm.conf"
      fi
      RANK=$((RANK+1))
    done <<< "$SKYPILOT_NODE_IPS"

    # Create partition definitions based on GPU availability
    gpu_nodes=""
    cpu_nodes=""
    
    for node_info in "${nodeinfo[@]}"; do
      node_name=$(echo "$node_info" | cut -d: -f1)
      node_gpus=$(echo "$node_info" | cut -d: -f2)
      
      if [ "$node_gpus" -gt 0 ]; then
        if [ -z "$gpu_nodes" ]; then
          gpu_nodes="$node_name"
        else
          gpu_nodes="$gpu_nodes,$node_name"
        fi
      fi
      
      if [ -z "$cpu_nodes" ]; then
        cpu_nodes="$node_name"
      else
        cpu_nodes="$cpu_nodes,$node_name"
      fi
    done
    
    # Create partitions
    sudo sh -c "echo \"PartitionName=debug Nodes=ALL Default=YES MaxTime=INFINITE State=UP\" >> /etc/slurm/slurm.conf"
    
    # Create GPU partition if GPU nodes exist
    if [ -n "$gpu_nodes" ]; then
      sudo sh -c "echo \"PartitionName=gpu Nodes=${gpu_nodes} Default=NO MaxTime=INFINITE State=UP\" >> /etc/slurm/slurm.conf"
    fi

    # Distribute slurm.conf and gres.conf to all nodes
    for node in $(echo "$SKYPILOT_NODE_IPS" | tail -n +2); do
      scp -o StrictHostKeyChecking=no /etc/slurm/slurm.conf $node:/tmp/
      ssh $node "sudo mv /tmp/slurm.conf /etc/slurm/slurm.conf"
      
      scp -o StrictHostKeyChecking=no /etc/slurm/gres.conf $node:/tmp/
      ssh $node "sudo mv /tmp/gres.conf /etc/slurm/gres.conf"
    done

    sudo systemctl stop slurmctld || true
    sudo systemctl enable slurmctld
    sudo systemctl start slurmctld
  fi
  # Create barrier to ensure slurm.conf is distributed before continuing
  create_barrier "slurm_conf_distributed"

  sudo mkdir -p /var/spool/slurmd
  sudo chown slurm:slurm /var/spool/slurmd
  sudo systemctl stop slurmd || true
  sudo systemctl enable slurmd
  sudo systemctl start slurmd
  
  # Show cluster configuration and GPU status
  sinfo
  if command -v nvidia-smi &> /dev/null; then
    echo "=== GPU Status ==="
    nvidia-smi
    echo "=== Available GPU Resources in Slurm ==="
    scontrol show nodes | grep "Gres="
  fi
  
  echo "=== Example GPU job submission ==="
  echo "To request a GPU job: sbatch --gres=gpu:1 --partition=gpu my_gpu_job.sh"
  echo "To request multiple GPUs: sbatch --gres=gpu:4 --partition=gpu my_gpu_job.sh"
