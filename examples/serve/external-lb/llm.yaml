name: multi-lb-llm

envs:
  # MODEL_NAME: meta-llama/Llama-3.2-3B-Instruct
  MODEL_NAME: meta-llama/Llama-3.1-8B-Instruct
  HF_TOKEN:

service:
  readiness_probe:
    path: /v1/models
    # path: /v1/chat/completions
    # post_data:
    #   model: $MODEL_NAME
    #   messages:
    #     - role: user
    #       content: Hello! What is your name?
    #   max_tokens: 1
  replicas: 4
  # TODO(tian): Change the config to a cloud-agnostic way.
  route53_hosted_zone: aws.cblmemo.net
  # Meta load balancing policy used to select the LB.
  # load_balancing_policy: proximate_first
  external_load_balancers:
    - resources:
        cloud: aws
        region: us-east-2
      load_balancing_policy: prefix_tree
    - resources:
        cloud: aws
        region: ap-northeast-1
      load_balancing_policy: prefix_tree
  # max_concurrent_requests: 50
  # max_queue_size: 10000

resources:
  cloud: aws
  accelerators: L4:1
  any_of:
    - region: us-east-2
    - region: ap-northeast-1
  ports: 8081

setup: |
  # pip install vllm==0.6.2
  pip install transformers==4.48.3 "sglang[all]==0.4.3.post2" \
    --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/

run: |
  # vllm serve $MODEL_NAME \
  #   --port 8081 \
  #   --enable-prefix-caching \
  #   --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
  #   --max-model-len 4096
  python -m sglang.launch_server \
    --model-path $MODEL_NAME \
    --port 8081 \
    --host 0.0.0.0 \
    --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
    --enable-metrics \
    --mem-fraction-static 0.8
